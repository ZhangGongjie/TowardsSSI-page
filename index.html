<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting</h1>
          <div class="is-size-5 publication-authors" style="margin-top: 2rem;">
            <span class="author-block">
              <a href="https://xingy038.github.io/">Xingyu Miao</a ><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://haoranduan.com/">Haoran Duan</a ><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Quanhao_Qian1">Quanhao Qian</a ><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jiuniu.ruantang.top/">Jiuniu Wang</a ><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=IrkuknEAAAAJ">Yang Long</a ><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=zh-CN">Ling Shao</a ><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhaodeli.github.io/">Deli Zhao</a ><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/Xuran0a444198/">Ran Xu</a><sup>2</sup><span style="font-size: 1.0em;">✉</span>,
            </span>
            <span class="author-block">
              <a href="https://zhanggongjie.github.io">Gongjie Zhang</a><sup>2 †</sup><span style="font-size: 1.0em;">✉</span>
            </span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top: 1.5rem;">
            <span class="author-block"><sup>1</sup>Durham University&nbsp;&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>DAMO Academy, Alibaba Group&nbsp;&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;<br>
            <span class="author-block"><sup>3</sup>Tsinghua University&nbsp;&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>4</sup>UCAS-Terminus AI Lab&nbsp;&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <div style="margin-top: 0.8rem; font-size: 0.85em; color: #666;">
              <sup>†</sup>Project Lead,  &nbsp; &nbsp;   <span style="font-size: 1.2em;">✉</span> Co-corresponding Authors
            </div>
          </div>

          <div class="is-size-6 has-text-centered" style="margin-top: 2rem;">
            <span class="tag is-light is-medium">ICCV 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="./coming-soon.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="./coming-soon.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code for Data Generation</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="./coming-soon.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation.
          </p>
          <p>
            In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations — including point clouds, camera poses, depth maps, and pseudo-RGBD — via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence.
          </p>
          <p>
            We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Motivation -->
    <div class="columns is-centered has-text-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Spatial intelligence is crucial for AI systems to perceive, understand, and interact with 3D environments. 
            However, unlike 2D imagery which is abundant online, 3D datasets are expensive to collect and require 
            specialized sensors like LiDAR. This creates a critical data bottleneck for advancing spatial intelligence.
          </p>
          <p>
            To address the limitations of existing spatial datasets, we present a novel data-generation pipeline that 
            <strong>lifts</strong> large-scale 2D image datasets into high-quality, richly annotated 3D representations 
            covering diverse scenes and tasks. This approach is superior to purely synthetic simulation data while avoiding 
            the high costs of sensor-based data collection.
          </p>
        </div>
      </div>
    </div>
    <!--/ Motivation. -->

    <!-- Data Curation Pipeline -->
    <div class="columns is-centered has-text-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data Curation Pipeline</h2>
        <img id="pipeline" src="./static/images/pipline.jpg" alt="Pipeline image" style="width: 100%; margin: 1.5rem 0;">
        <div class="content has-text-justified">
          <p>
            Our pipeline converts single-view 2D images into comprehensive 3D representations through three key steps:
          </p>
          <p>
            <strong>Step 1:</strong> We generate a scale-calibrated depth map by integrating scale-invariant and 
            scale-aware depth estimation techniques. This ensures accurate geometric reconstruction with proper scale information. 
            (Author's Note: We noticed recent works (VGGT, UniDepth-V2, etc.) have even better results for depth estimation. We believe our pipeline can be improved by using these methods.)
          </p>
          <p>
            <strong>Step 2:</strong> Predicted camera parameters are used to project images into 3D space and 
            remove invalid points, creating accurate point cloud representations of the scene.
          </p>
          <p>
            <strong>Step 3:</strong> Original 2D image annotations are lifted to 3D, resulting in a fully annotated 
            3D representation ready for various downstream spatial intelligence tasks.
          </p>
        </div>

        <img id="teaser" src="./static/images/teaser.jpg" alt="Teaser image" style="width: 100%; margin: 1.5rem 0;">
        <p class="has-text-centered" style="font-style: italic; color: #666; margin-top: 0.5rem;">
        </p>
        <p>
          Our pipeline mitigates the scarcity of spatial data by generating scale- and metric-authentic 3D data 
          (point clouds, depth maps, camera poses, etc.) with rich annotations. Our generated data can support 
          a wide range of tasks, including spatial perception and MLLM-based captioning, spatial reasoning, and grounding.
        </p>

      </div>
    </div>
    <!--/ Data Curation Pipeline. -->

    <!-- Experiments -->
    <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p style="text-align: center; color: #666; font-style: italic;">
            Detailed experimental results and evaluations will be added soon.
          </p>
        </div>
      </div>
    </div>
    <!--/ Experiments. -->

    <!-- Demos -->
    <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demos for Generated 3D Data (with Annotations)</h2>
        <div class="publication-video">
              <video id="paper" autoplay muted loop playsinline height="100%" width="100%">
              <source src="./static/videos/video.mp4" type="video/mp4">
      </video>
        </div>
      </div>
    </div>
    <!--/ Demos. -->
  </div>
</section>

  </div>
</section>


<section class="section" id="BibTeX" style="margin-top: 3rem;">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
      <pre>
        <code>
@inproceedings{miao2025towards,
  title={Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting},
  author={Miao, Xingyu and Duan, Haoran and Qian, Quanhao and Wang, Jiuniu and Long, Yang and Shao, Ling and Zhao, Deli and Xu, Ran and Zhang, Gongjie},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}
      </code>
    </pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/ZhangGongjie" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/ZhangGongjie/TowardsSSI-page">source code</a> of this website,
            we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
